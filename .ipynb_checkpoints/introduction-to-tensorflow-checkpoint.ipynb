{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Tensorflow? Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Forward algorithm - we know how to do this, fairly straightforward matrix multiplication.\n",
    "  * Backward operation: Chain Rule. We know how to do this. We can compute partial derivatives of the loss with respect to each weight. It takes a little thought, but we have also implemented gradient calculations with fairly straightforward matrix multiplications. \n",
    "  * We can do this manually, but it's obnoxious. How could we automate this? Store the whole computation plan as a graph \n",
    "      * We'll use a graph to store the whole plan for how to compute anything in our program. The graph is made of basic primitive objects (tensors and ops) that define the quantities we care about and how to compute some components from others.\n",
    "      * Everything we care about can be stored in a particular type of graph called a Directed Acylic Graph. \n",
    "      * Using the primitives and the relationships defined in the DAG, we can automate the process of computing and relationship between variables - relationships such as partial derivatives.\n",
    "      \n",
    "* Read more about AutoDiff - https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32, shape=(1))\n",
    "b = tf.placeholder(tf.float32, shape=(1))\n",
    "\n",
    "c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, nothing _really_ happens yet. TF constructs the model graph in the background, but we haven't evaluated any part of it and we haven't filed in any of the placeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate any part of a TF graph, we need to execute one of the graphs nodes inside of a TF session, using the method session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(c)\n",
    "    \n",
    "# Big error here, because we can't just evaluate the object 'c' since we don't have any real values for 'a' and 'b'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At evaluation time, we will supply the required values for any needed placeholders. We do with with a special argument called feed_dict. This is a dictionary whose keys should match the actual names of nodes in the graph and whose values will be various arrays that can supply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 11.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\n",
    "        sess.run(c, feed_dict={a:np.array([5]),b:np.array([6])})\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the evaluated value of the node 'c'.\n",
    "\n",
    "Note, we only filled in the actual values of 'a' and 'b' when we want to do session.run(). And when we do this, we provide regular old numpy arrays and then Tensorflow will convert these into its own datatypes that it prefers to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a special type of node for representing constant values that we know ahead of time and we know will never change. You might not use them very often, but here they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(dtype=tf.float32, value= 5)\n",
    "y = tf.placeholder(dtype=tf.float32, shape=(1))\n",
    "z = x + y\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\n",
    "        sess.run(z, feed_dict={y:np.array([3])})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we only had to provide the value of 'y' in our feed_dict. That's because the node 'x' is constant and we've already specified its value and its value will not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why is it useful to have placeholders for modeling data? Can't we use constant tensors for that, since the data is fixed?\n",
    "\n",
    "_Hint_: Think about feeding data batches into a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we know we can use constants to represent values that never change. And we can use placeholders to represent values that won't be supplied until later, but will never be updated during the course of model fitting. What do we do to represent values that _do_ change, the free parameters of the model? For this, we use the Variable class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = tf.Variable(tf.random_normal([2, 2], stddev=0.35),name=\"m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create this Variable, we have to supply a way of giving it an intial value. In this case, we want the initial values to be Normally distributed. Further, the method we use for creating that intial values also serves to let us define the _shape_ of this tensor. In this case, we want a 2x2 tensor whose intial values are Normally distributed with mean 0 and standard deviation 0.35.\n",
    "\n",
    "So let's take a look at this tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\n",
    "        sess.run(m)\n",
    "    )\n",
    "# Nope, this won't work.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above won't work. The reason is that even though we've supplied the _process_ for initializing our tensor, we have only created the computation graph. We haven't actaully kicked off the initialization procedure to supply any real values - we've only specific _how_ to do that. So with Variables, we need to not only describe _how_ they are initialized, but we need to actaully do so. The most common way is to use tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.26620319  0.25507692]\n",
      " [ 0.03349717 -0.08588654]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer() )\n",
    "        print(sess.run(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, we'll use the Variable class to represent Tensors of free parameters whose values we adjust through model training. Much like placeholders and constants, we have to provide information about the shape of the tensor. But we must also provide instructors about what the values of the Variable tensor should be intialized with. For this, we'll usually use some built-in methods such as tf.zeros_initializer() or tf.random_normal() or tf.random_uniform(). And finally, you can't forget to actually kick off the initial values of your Variables by using tf.global_variables_initializer()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement interesting machine learning models, we typically need to perform arithmetic operations on numeric arrays. We often need simple things like vector and matrix addition and multipication, dot products, sums and averages and so on. So far, we've talked about the _nodes_ of our graphs, which is nearly all cases are tensors of some shape. But to do anything interesting (like define a model) we have to be able to combine those tensors and operate on them. All the arithmetic operations we might ever want are defined in tensorflow, and we have to make sure that we make use tensorflow's version of these things. Tensorflow refers to all such operations as \"ops\". \n",
    "\n",
    "We have to be sure to define our models in terms of Tensorflow ops and not in terms of numpy operations or base python operations. This is because Tensorflow can combine ops with Tensors to acheive the automatic differentiation we talked about earlier. Here's a good list of possible operations to get you started https://www.tensorflow.org/versions/r1.3/api_guides/python/math_ops#Matrix_Math_Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(dtype=tf.float32, value= np.array([[1, 2], [2, 1]]))\n",
    "y = tf.constant(dtype=tf.float32, value= np.array([[5, 5], [5, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix x:\n",
      "[[ 1.  2.]\n",
      " [ 2.  1.]]\n",
      "\n",
      "matrix y:\n",
      "[[ 5.  5.]\n",
      " [ 5.  5.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"matrix x:\\n{}\\n\".format(sess.run(x)))\n",
    "    print(\"matrix y:\\n{}\\n\".format(sess.run(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to multiply a couple of matrices, we'll use tf.matml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix xy:\n",
      "[[ 15.  15.]\n",
      " [ 15.  15.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"matrix xy:\\n{}\\n\".format(sess.run(tf.matmul(x,y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly every arithmetic operation you might do to a tensor must be done with a Tensorflow op. Confusingly (or conveniently), there's a couple of common in-fix operators that are (through some python magic) converted into Tensorflow ops. Just try to keep an eye out - keep in the mind that most tensor operations need to be accomplished through special tf methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix xy:\n",
      "[[ 6.  7.]\n",
      " [ 7.  6.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"matrix xy:\\n{}\\n\".format(sess.run(x + y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a couple other kinds of ops that are useful to know about. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aggregate over a vector, you can use \"reduce\" type functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = tf.constant(dtype=tf.float32, value=np.array([2,3,4,5,6,7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of vector: 27.0\n",
      "min of vector: 2.0\n",
      "max of vector: 7.0\n",
      "mean of vector: 4.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\n",
    "    \"sum of vector: {}\\n\".format(sess.run( tf.reduce_sum(vec) ))\n",
    "    + \"min of vector: {}\\n\".format(sess.run( tf.reduce_min(vec) ))\n",
    "    + \"max of vector: {}\\n\".format(sess.run( tf.reduce_max(vec) ))\n",
    "    + \"mean of vector: {}\\n\".format(sess.run( tf.reduce_mean(vec) ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's functions for indexing into tensors in interesting ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [4]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print( sess.run(tf.where(vec > 4)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply TF's mathematical functions element-wise over tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.41614681 -0.9899925  -0.65364361  0.28366217  0.96017027  0.75390226]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print( sess.run(tf.cos(vec)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More commonly, would probably be some non-linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh activation: [ 0.96402758  0.99505472  0.99932921  0.99990916  0.99998784  0.99999833]\n",
      "relu activation: [ 2.  3.  4.  5.  6.  7.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\n",
    "    \"tanh activation: {}\\n\".format(sess.run( tf.nn.tanh(vec) ))\n",
    "    + \"relu activation: {}\\n\".format(sess.run( tf.nn.relu(vec) ))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned about the basic peices of Tensorflow, we're going to put them together into our first model.\n",
    "\n",
    "By they way, if you continue to use jupyter notebooks for developing Tensorflow code, you can avoid all the tf.Session() boilerplate by using tf.InteractiveSession() \n",
    "\n",
    "https://www.tensorflow.org/versions/r0.12/api_docs/python/client/session_management#InteractiveSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    " * create 3x3 matrix called A. Let its initial values be Normally distributed.\n",
    " * create a 3x3 matrix called B. Let its intial values be Uniformly distributed.\n",
    " * create a matrix C which is the product of A times B\n",
    " * create a matrix D which applies the Relu nonlinearity to every element of C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Defining a model DAG: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's dive in and code up our first example model with Tensorflow. In this example, we'll use classic multiple linear regression to fit some continuous target variable from some continuous predictor variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{Y} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create some training data, just using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.multivariate_normal(mean=[0,0], cov=[[1,0],[0,3]], size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_0 = 5\n",
    "beta_1 = 2\n",
    "beta_2 = -3\n",
    "\n",
    "Y = beta_0 + beta_1*X[:,0] + beta_2 * X[:,1] + np.random.normal(size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our \"design\" matrix, which is just our data matrix X, with an addition column of 1's. This additional column of 1's accounts for the constant term beta_0. This allows us to rewrite the above as a single matrix operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = X \\beta + \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where X is a 1000x3 matrix, and Beta is a 3x1 vector, yield Y which is a 1000x1 vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_design_mat = np.ones((1000,3))\n",
    "X_design_mat[:,1:] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the value of the Beta vector that minimizes the error betwee Y_hat and the true Y. You might remember that under certain common assumptions, the optimal estimate of Beta is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{\\beta} = (X^TX)^{-1}X^TY $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def OLS(x_mat, y_vec):\n",
    "    return np.dot(np.linalg.inv(np.dot(x_mat.transpose(), x_mat)),  np.dot(x_mat.transpose(), y_vec) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's just sanity check and make sure we can recover our known beta values, given this analytical solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.96865609,  1.9870594 , -3.02072595])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat = OLS(X_design_mat,Y)\n",
    "beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, so we'll next construct this model with Tensorflow. Instead of using the analytical solution to find the betas, we will use Tensorflow's optmization and gradient descent techniques. We will hope that TF is able to come to a similar answer as we were."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create placeholder tensors where we will put our actual observations. Our observations are two dimensional and our target variable is one dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape=(None,2))\n",
    "y_input = tf.placeholder(tf.float32, shape=(None,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what we're setting for shape. For the \"number of rows\" of the data, we're setting it to None. This is a cool trick, and the main reason we're using placeholders: we don't have to decide right now about how many data points are going to be fed into the model. At model evaluation time, we can experiment with different batch sizes. Our tensor for X is 2-dimensional, but can accomodate any number of data samples that we decide later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our model's free parameters. Since our model has only 3 free parameters, we will proceed by using scalar Variable objects. But most typically, we'll have large arrays of free parameters and see many examples of that in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta_0 = tf.Variable(tf.random_normal([1]), name = \"beta0\")\n",
    "beta1_and_beta2 = tf.Variable(tf.random_normal([2, 1]), name = \"beta1_and_beta2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model's predictions, which we'll call y_hat, are just a linear transform of the inputs, using our beta weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = tf.add(beta_0, tf.matmul(x_input, beta1_and_beta2)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate what is the predicted y_hat for each datapoint, given the observations X1 and X2 and the betas. Let's do this for just a handful of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.71725643]\n",
      " [-2.83219719]\n",
      " [-2.46513844]\n",
      " [-1.36858797]\n",
      " [-0.06228423]]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 5\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer() )\n",
    "    print(\n",
    "    sess.run(y_hat, \n",
    "             feed_dict = {x_input: X[:sample_size,:], \n",
    "                          y_input: Y[:sample_size,].reshape((sample_size,1))})\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our model's prediction for 5 input data points. This prediction comes from multiplying X1 and X2 with our beta weights (a simple 2x1 vector) and then adding our constant term beta_0. Since our beta weights are initialized randomly, this prediction is obviously way different than the ground truth Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.42276035,  11.45208296,   5.16420307,  11.61615396,   9.15810616])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next thing we need to do is define way for our model's free parameters to be updated so that they match the data closely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll devise a loss function that represents how different our model's prediction is from the ground truth. Then we'll use Tensorflow's built-in optimization methods in order to adjust the beta weights until the loss function is minimized. \n",
    "\n",
    "For our loss function, let's just use the sum-squared-error between y_hat and Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss =  tf.reduce_sum(tf.square(y_hat - y_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this object is just another node in the graph. In in order to be computed, it relies on other nodes such as y_hat and Y. And it relies on Tensorflow functions such as reduce_sum. Since Tensorflow is keeping track of all these objects in the graph, it is straightforward (though tedious) to understand the relationships between any of the objects in the graph.\n",
    "\n",
    "Specifically, training a model is akin to finding the values of the free parameters that result in the smallest loss. In order to do that, we'd need to understand the impact on our 'loss' object if we were to change a node such as 'beta0'. Mathematically, we'd like to know:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial \\beta_0}, \\frac{\\partial L}{\\partial \\beta_1}, \\frac{\\partial L}{\\partial \\beta_2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can the descend the gradient of the Loss surface until we find the minimum point. This would give us our best estimate of the betas and our best model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *could* calculate this by hand using the chain rule. In this simple case, it wouldn't be too bad. But the point of Tensorflow is automatic differentiation. Since everything in our model is a node or an op (whose derivatives are known), it is straightforward (though boring and exhausting) to compute the partial derivatives between any two nodes in the graph. So Tensorflow does this for us. Whether we have 3 parameters or 3 million parameters, this automatic differentiation is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll pick an optimizer to use for finding the minimum value of the 'loss' node. For simplicity, let's use gradient descent with a step size of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've now created a new node in the graph called \"training_step\". I instantiated a \"GradientDescentOptimizer\" and stated that I want to minimize the \"loss\" node in the graph. The node \"training_step\" doesn't store a particular value, but instead can be called at any time to perform a single step of gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our \"training_step\" is the thing that minimizes the loss function, we simply have to call it again and again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current value of Loss: 21695.4296875\n",
      "\n",
      "Current value of Loss: 12832.9609375\n",
      "\n",
      "Current value of Loss: 8284.84179688\n",
      "\n",
      "Current value of Loss: 5562.74316406\n",
      "\n",
      "Current value of Loss: 3867.03369141\n",
      "\n",
      "Current value of Loss: 2800.72314453\n",
      "\n",
      "Current value of Loss: 2128.62475586\n",
      "\n",
      "Current value of Loss: 1704.67358398\n",
      "\n",
      "Current value of Loss: 1437.13769531\n",
      "\n",
      "Current value of Loss: 1268.24975586\n",
      "\n",
      "Current value of Loss: 1161.60058594\n",
      "\n",
      "Current value of Loss: 1094.23217773\n",
      "\n",
      "Current value of Loss: 1051.66333008\n",
      "\n",
      "Current value of Loss: 1024.75549316\n",
      "\n",
      "Current value of Loss: 1007.74243164\n",
      "\n",
      "Estimates of model parameters:\n",
      "Beta0 parameter[ 4.84821796]\n",
      "Beta1&Beta2 parameter[[ 1.92163813]\n",
      " [-3.01736498]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict={x_input:X, y_input:Y.reshape(1000,1)}\n",
    "\n",
    "    for i in range(15):\n",
    "        sess.run(training_step, feed_dict=feed_dict)\n",
    "        print(\"Current value of Loss: {}\\n\".format(sess.run(loss, feed_dict=feed_dict)))\n",
    "        \n",
    "    print(\"Estimates of model parameters:\")\n",
    "    print(\"Beta0 parameter{}\".format(sess.run(beta_0, feed_dict=feed_dict)))\n",
    "    print(\"Beta1&Beta2 parameter{}\".format(sess.run(beta1_and_beta2, feed_dict=feed_dict)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encouragingly, we were able to get pretty accurate estimates of the model parameters just by doing gradient descent. With auto-differentiation, we can rely on gradient descent (and other optimization techniques) to fit our models for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    " * Try this again but make changes to the optimization and observe what happens. In particular:\n",
    "    * Make the Optimizer's learning rate larger, something like 1 or larger. What happens?\n",
    "    * Make the Optimizer's learning rate smaller, something like 0.000001 or lower. What happens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to any other kinds of models, let's explore a useful utility that comes with Tensorflow. Tensorboard is a visualization suite that allows up to follow the progress of model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get things set up for Tensboard, we have to set up a location where our models will be saving various metadata. Then we have make some changes to our code to specify what should be saved and when. \n",
    "\n",
    "I'll create a directory called `tensorboard-logs` in this current directory. We'll refer to this directory throughout our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The following is a trick we have to use to get the full path to the current jupyer notebook. You can ignore. You can hard-code in the path to the tensorboard-log directory if you like.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var command = \"nb_name = '\" + IPython.notebook.base_url + \"'; nb_name = nb_name.split('/')[-1]\";\n",
       "IPython.notebook.kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var command = \"nb_name = '\" + IPython.notebook.base_url + \"'; nb_name = nb_name.split('/')[-1]\";\n",
    "IPython.notebook.kernel.execute(command);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we just specify the tensorboard-log directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.path.join(os.getcwd(), nb_name)\n",
    "LOG_DIR = os.path.join(pwd, \"tensorboard-logs\")\n",
    "os.mkdir(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a SummaryWriter object. This writer object will be our interface to save things to the tensorboard-logs directory. When we first create it, we don't have much to save, but we will save current state of the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(LOG_DIR, graph=sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can open up tensorboard and start looking around. Tensorboard is a separate process that we have to start from the terminal. The command should look something like\n",
    "\n",
    "`tensorboard --logdir ./tensboard-logs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will serve tensorboard probably on port 6006, so open a web browser and go to http://localhost:6006 \n",
    "\n",
    "We should see tensorboard! At this point, the only thing we can explore is the computation graph in the \"Graph\" tab. Take a look around. You'll see that every node and op, every placeholder and variable, is in this graph. In fact, there's more nodes that we didn't specify, but that Tensorflow implicitly creates as part of optimization. Indeed, the storage and traversal of this computation graph is the main powerhouse behind tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add more information to Tensorboard. The strategy is that we can periodically save the state or value of anything in our model and Tensorboard will display it for us. We'll use our Writer objects to do the saving. So for any quantity that we want to save metrics for, we'll create a \"Sumamry\" object in our model.\n",
    "\n",
    "At this time, our TF session graph is getting a little cluttered because we keep running the same code repeatedly in our notebook. This would be a good time to restart the kernel for the notebook and refresh things. Then we'll just use the model code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_input = tf.placeholder(tf.float32, shape=(None,2))\n",
    "y_input = tf.placeholder(tf.float32, shape=(None,1))\n",
    "\n",
    "beta0 = tf.Variable(tf.random_normal([1]), name = \"beta0\")\n",
    "#tf.summary.scalar('beta0', beta0)\n",
    "beta1_and_beta2 = tf.Variable(tf.random_normal([2, 1]), name = \"beta1_and_beta2\")\n",
    "\n",
    "y_hat = tf.add(beta0, tf.matmul(x_input, beta1_and_beta2)  )\n",
    "\n",
    "loss =  tf.reduce_sum(tf.square(y_hat - y_input))\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "training_step = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some training data\n",
    "X = np.random.multivariate_normal(mean=[0,0], cov=[[1,0],[0,3]], size = 1000)\n",
    "beta_0, beta_1, beta_2 = 5, 2, -3\n",
    "\n",
    "Y = beta_0 + beta_1*X[:,0] + beta_2 * X[:,1] + np.random.normal(size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's new here is that we've added tf.summary.scalar() methods. This declares a summary object for certain scalars in our graph. Then, at model training time, we can use our Writer object to save the values of those scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    RUN_DIR = os.path.join(LOG_DIR, str(int(time.time())))\n",
    "    writer = tf.summary.FileWriter(RUN_DIR, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    feed_dict={x_input:X, y_input:Y.reshape(1000,1)}\n",
    "\n",
    "    for i in range(35):\n",
    "        _, summary = sess.run([training_step, merged_summaries], feed_dict=feed_dict)\n",
    "        writer.add_summary(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we look at tensorboard, we should see the \"scalars\" tab. The shows us the evolution of our scalar summary variables throughout the course of model training. In fact, if we run model training multiple times, or with slight variations of the models, we can visualize all these traces at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let's repeat the exercise we just did where we are varying the value of the learning rate parameter. But this time, we'll be saving the loss value out for Tensorboard. Traing the model three times, each time with a different value for the learning rate. Go to a look in Tensorboard and the visual representation of the learning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting up output directories for logging\n",
    "* TF summary objects (maybe a summary for loss and a summary for one of the params. Maybe histograms for all the params?)\n",
    "* model checkpointing\n",
    "* embedding viz (nah, wouldnt make sense at this point in the course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've just done linear regression, it's up to you to try another model. We'll move to binary classification models and implement Logistic Regression. Lots of things will be similar to our previous code, but there's a few important differences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " * Which mathematical components will be unchanged between linear regression and logistic regression?\n",
    " * Which are the primary changes you'll have to make in order to implement logistic regression?\n",
    " * Implement a logistic regression model and use it to fit the supplied data set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
